{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import neuron, Layer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)]],\n",
       " [Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0)])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = [random.choice([0,1]) for _ in range(20)]\n",
    "X2 = [random.choice([0,1]) for _ in range(20)]\n",
    "X = [[Value(x1),Value(x2)] for x1,x2 in zip(X1,X2)]\n",
    "y =[Value((x1!=x2)*1) for x1,x2 in zip(X1,X2)]\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp:\n",
    "    def __init__(self):\n",
    "        self.layers = [Layer(2,8), neuron(8, nonlin=False)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 0.23661940504561035\n",
      "epoch 1 loss: 0.23320537555609522\n",
      "epoch 2 loss: 0.22981383151606644\n",
      "epoch 3 loss: 0.22644525993408732\n",
      "epoch 4 loss: 0.22310013380711122\n",
      "epoch 5 loss: 0.21977891195366805\n",
      "epoch 6 loss: 0.21648203886441297\n",
      "epoch 7 loss: 0.21320994456975617\n",
      "epoch 8 loss: 0.20996304452427453\n",
      "epoch 9 loss: 0.20674173950759037\n",
      "epoch 10 loss: 0.2035464155413852\n",
      "epoch 11 loss: 0.20037744382220354\n",
      "epoch 12 loss: 0.1972351806696921\n",
      "epoch 13 loss: 0.19411996748991106\n",
      "epoch 14 loss: 0.19103213075334155\n",
      "epoch 15 loss: 0.18797198198721693\n",
      "epoch 16 loss: 0.18493981778179205\n",
      "epoch 17 loss: 0.18193591981016485\n",
      "epoch 18 loss: 0.17896055486126608\n",
      "epoch 19 loss: 0.17601397488562537\n",
      "epoch 20 loss: 0.17309641705352635\n",
      "epoch 21 loss: 0.17020810382516377\n",
      "epoch 22 loss: 0.16734924303241452\n",
      "epoch 23 loss: 0.16452002797183635\n",
      "epoch 24 loss: 0.16172063750851554\n",
      "epoch 25 loss: 0.1589512361903783\n",
      "epoch 26 loss: 0.15621197437259163\n",
      "epoch 27 loss: 0.15350298835167733\n",
      "epoch 28 loss: 0.15082440050896767\n",
      "epoch 29 loss: 0.1481763194630362\n",
      "epoch 30 loss: 0.14555884023073515\n",
      "epoch 31 loss: 0.1429720443964817\n",
      "epoch 32 loss: 0.14041600028942974\n",
      "epoch 33 loss: 0.1378907631681754\n",
      "epoch 34 loss: 0.13539637541264243\n",
      "epoch 35 loss: 0.13293286672279722\n",
      "epoch 36 loss: 0.13050025432384868\n",
      "epoch 37 loss: 0.12809854317758781\n",
      "epoch 38 loss: 0.1257277261995285\n",
      "epoch 39 loss: 0.1233877844815095\n",
      "epoch 40 loss: 0.12107868751942526\n",
      "epoch 41 loss: 0.11880039344575144\n",
      "epoch 42 loss: 0.11655284926653801\n",
      "epoch 43 loss: 0.11433599110254232\n",
      "epoch 44 loss: 0.11214974443418024\n",
      "epoch 45 loss: 0.10999402434997502\n",
      "epoch 46 loss: 0.10786873579818647\n",
      "epoch 47 loss: 0.10577377384130826\n",
      "epoch 48 loss: 0.10370902391312166\n",
      "epoch 49 loss: 0.10167436207800155\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    #one epoch of training\n",
    "    clf.zero_grad()\n",
    "    loss = Value(0)\n",
    "    for i in range(len(X)):\n",
    "        out = clf(X[i])\n",
    "        loss = loss + (-y[i] + out)*(-y[i] + out)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #update paramters\n",
    "\n",
    "    for p in clf.parameters():\n",
    "        p.data += -0.01*p.grad\n",
    "\n",
    "    print(f'epoch {_} loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.7097991388523793, grad=0)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf([1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
