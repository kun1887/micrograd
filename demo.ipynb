{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import neuron, Layer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[Value(data=1, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=1, grad=0), Value(data=0, grad=0)],\n",
       "  [Value(data=0, grad=0), Value(data=1, grad=0)]],\n",
       " [Value(data=1, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=0, grad=0),\n",
       "  Value(data=1, grad=0),\n",
       "  Value(data=1, grad=0)])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a tiny XOR dataset\n",
    "X1 = [random.choice([0,1]) for _ in range(20)]\n",
    "X2 = [random.choice([0,1]) for _ in range(20)]\n",
    "X = [[Value(x1),Value(x2)] for x1,x2 in zip(X1,X2)]\n",
    "y =[Value((x1!=x2)*1) for x1,x2 in zip(X1,X2)]\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mlp:\n",
    "    def __init__(self):\n",
    "        self.layers = [Layer(2,8), neuron(8, nonlin=False)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss: 18.48226252132663\n",
      "epoch 20 loss: 1.0660252862941315\n",
      "epoch 40 loss: 0.47350546745708216\n",
      "epoch 60 loss: 0.22303368540645688\n",
      "epoch 80 loss: 0.10228246217059639\n",
      "epoch 100 loss: 0.04645297064959541\n",
      "epoch 120 loss: 0.02064193565937767\n",
      "epoch 140 loss: 0.009165113233882999\n",
      "epoch 160 loss: 0.003985699294053325\n",
      "epoch 180 loss: 0.0017283713899249248\n"
     ]
    }
   ],
   "source": [
    "#training a nn with 1 hidden layer to learn XOR\n",
    "\n",
    "for _ in range(200):\n",
    "    #one epoch of training\n",
    "    clf.zero_grad()\n",
    "    loss = Value(0)\n",
    "    for i in range(len(X)):\n",
    "        out = clf(X[i])\n",
    "        loss = loss + (-y[i] + out)*(-y[i] + out)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    #update paramters\n",
    "    for p in clf.parameters():\n",
    "        p.data += -0.01*p.grad\n",
    "\n",
    "    if _ % 20 == 0:\n",
    "        print(f'epoch {_} loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9956261268869936,\n",
       " 0.9980805351568673,\n",
       " 0.0011744151495374985,\n",
       " 0.012476701303970338)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"\"validatio\"\"\n",
    "clf([1,0]).data, clf([0,1]).data, clf([1,1]).data, clf([0,0]).data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
